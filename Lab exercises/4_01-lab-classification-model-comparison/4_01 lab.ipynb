{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4, Lab 1: Predicting Left-Handedness from Psychological Factors\n",
    "> Author: Matt Brems\n",
    "\n",
    "We can sketch out the data science process as follows:\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "We'll walk through a full data science problem in this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Define The Problem.\n",
    "\n",
    "You're currently a data scientist working at a university. A professor of psychology is attempting to study the relationship between personalities and left-handedness. They have tasked you with gathering evidence so that they may publish.\n",
    "\n",
    "Specifically, the professor says \"I need to prove that left-handedness is caused by some personality trait. Go find that personality trait and the data to back it up.\"\n",
    "\n",
    "As a data scientist, you know that any real data science problem must be **specific** and **conclusively answerable**. For example:\n",
    "- Bad data science problem: \"What is the link between obesity and blood pressure?\"\n",
    "    - This is vague and is not conclusively answerable. That is, two people might look at the conclusion and one may say \"Sure, the problem has been answered!\" and the other may say \"The problem has not yet been answered.\"\n",
    "- Good data science problem: \"Does an association exist between obesity and blood pressure?\"\n",
    "    - This is more specific and is conclusively answerable. The problem specifically is asking for a \"Yes\" or \"No\" answer. Based on that, two independent people should both be able to say either \"Yes, the problem has been answered\" or \"No, the problem has not yet been answered.\"\n",
    "- Excellent data science problem: \"As obesity increases, how does blood pressure change?\"\n",
    "    - This is very specific and is conclusively answerable. The problem specifically seeks to understand the effect of one variable on the other.\n",
    "\n",
    "### 1. In the context of the left-handedness and personality example, what are three specific and conclusively answerable problems that you could answer using data science? \n",
    "\n",
    "> You might find it helpful to check out the codebook in the repo for some inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answers:__\n",
    "\n",
    "1. Is a person who day-dreams, more likely to be left-handed? <br>\n",
    "    _Q5_ \n",
    "1. Is a person who puts on fake concerts as a child, more likely to be left-handed? <br>\n",
    "    _Q16_\n",
    "1. Is a person who jumps when excited, more likely to be left-handed? <br>\n",
    "    _Q26_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Obtain the data.\n",
    "\n",
    "### 2. Read in the file titled \"data.csv.\"\n",
    "> Hint: Despite being saved as a .csv file, you won't be able to simply `pd.read_csv()` this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  country  fromgoogle  engnat  \\\n",
       "0   4   1   5   1   5   1   5   1   4    1  ...       US           2       1   \n",
       "1   1   5   1   4   2   5   5   4   1    5  ...       CA           2       1   \n",
       "\n",
       "   age  education  gender  orientation  race  religion  hand  \n",
       "0   22          3       1            1     3         2     3  \n",
       "1   14          1       2            2     6         1     1  \n",
       "\n",
       "[2 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppose that, instead of us giving you this data in a file, you were actually conducting a survey to gather this data yourself. From an ethics/privacy point of view, what are three things you might consider when attempting to gather this data?\n",
    "> When working with sensitive data like sexual orientation or gender identity, we need to consider how this data could be used if it fell into the wrong hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Answer:__**\n",
    "\n",
    "1. When collecting data, need to explain to participants why these particular data points are being collected and how would they relate to what the study is trying to achieve\n",
    "2. How the data would be de-identified, so that no particular data point can be traced back to an individual\n",
    "3. How long will the data be kept, and timeframe that it would disposed of in a secure manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Explore the data.\n",
    "\n",
    "### 4. Conduct exploratory data analysis on this dataset.\n",
    "> If you haven't already, be sure to check out the codebook in the repo, as that will help in your EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4184 entries, 0 to 4183\n",
      "Data columns (total 56 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Q1           4184 non-null   int64 \n",
      " 1   Q2           4184 non-null   int64 \n",
      " 2   Q3           4184 non-null   int64 \n",
      " 3   Q4           4184 non-null   int64 \n",
      " 4   Q5           4184 non-null   int64 \n",
      " 5   Q6           4184 non-null   int64 \n",
      " 6   Q7           4184 non-null   int64 \n",
      " 7   Q8           4184 non-null   int64 \n",
      " 8   Q9           4184 non-null   int64 \n",
      " 9   Q10          4184 non-null   int64 \n",
      " 10  Q11          4184 non-null   int64 \n",
      " 11  Q12          4184 non-null   int64 \n",
      " 12  Q13          4184 non-null   int64 \n",
      " 13  Q14          4184 non-null   int64 \n",
      " 14  Q15          4184 non-null   int64 \n",
      " 15  Q16          4184 non-null   int64 \n",
      " 16  Q17          4184 non-null   int64 \n",
      " 17  Q18          4184 non-null   int64 \n",
      " 18  Q19          4184 non-null   int64 \n",
      " 19  Q20          4184 non-null   int64 \n",
      " 20  Q21          4184 non-null   int64 \n",
      " 21  Q22          4184 non-null   int64 \n",
      " 22  Q23          4184 non-null   int64 \n",
      " 23  Q24          4184 non-null   int64 \n",
      " 24  Q25          4184 non-null   int64 \n",
      " 25  Q26          4184 non-null   int64 \n",
      " 26  Q27          4184 non-null   int64 \n",
      " 27  Q28          4184 non-null   int64 \n",
      " 28  Q29          4184 non-null   int64 \n",
      " 29  Q30          4184 non-null   int64 \n",
      " 30  Q31          4184 non-null   int64 \n",
      " 31  Q32          4184 non-null   int64 \n",
      " 32  Q33          4184 non-null   int64 \n",
      " 33  Q34          4184 non-null   int64 \n",
      " 34  Q35          4184 non-null   int64 \n",
      " 35  Q36          4184 non-null   int64 \n",
      " 36  Q37          4184 non-null   int64 \n",
      " 37  Q38          4184 non-null   int64 \n",
      " 38  Q39          4184 non-null   int64 \n",
      " 39  Q40          4184 non-null   int64 \n",
      " 40  Q41          4184 non-null   int64 \n",
      " 41  Q42          4184 non-null   int64 \n",
      " 42  Q43          4184 non-null   int64 \n",
      " 43  Q44          4184 non-null   int64 \n",
      " 44  introelapse  4184 non-null   int64 \n",
      " 45  testelapse   4184 non-null   int64 \n",
      " 46  country      4184 non-null   object\n",
      " 47  fromgoogle   4184 non-null   int64 \n",
      " 48  engnat       4184 non-null   int64 \n",
      " 49  age          4184 non-null   int64 \n",
      " 50  education    4184 non-null   int64 \n",
      " 51  gender       4184 non-null   int64 \n",
      " 52  orientation  4184 non-null   int64 \n",
      " 53  race         4184 non-null   int64 \n",
      " 54  religion     4184 non-null   int64 \n",
      " 55  hand         4184 non-null   int64 \n",
      "dtypes: int64(55), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1             0.0\n",
       "Q2             0.0\n",
       "Q3             0.0\n",
       "Q4             0.0\n",
       "Q5             0.0\n",
       "Q6             0.0\n",
       "Q7             0.0\n",
       "Q8             0.0\n",
       "Q9             0.0\n",
       "Q10            0.0\n",
       "Q11            0.0\n",
       "Q12            0.0\n",
       "Q13            0.0\n",
       "Q14            0.0\n",
       "Q15            0.0\n",
       "Q16            0.0\n",
       "Q17            0.0\n",
       "Q18            0.0\n",
       "Q19            0.0\n",
       "Q20            0.0\n",
       "Q21            0.0\n",
       "Q22            0.0\n",
       "Q23            0.0\n",
       "Q24            0.0\n",
       "Q25            0.0\n",
       "Q26            0.0\n",
       "Q27            0.0\n",
       "Q28            0.0\n",
       "Q29            0.0\n",
       "Q30            0.0\n",
       "Q31            0.0\n",
       "Q32            0.0\n",
       "Q33            0.0\n",
       "Q34            0.0\n",
       "Q35            0.0\n",
       "Q36            0.0\n",
       "Q37            0.0\n",
       "Q38            0.0\n",
       "Q39            0.0\n",
       "Q40            0.0\n",
       "Q41            0.0\n",
       "Q42            0.0\n",
       "Q43            0.0\n",
       "Q44            0.0\n",
       "introelapse    0.0\n",
       "testelapse     0.0\n",
       "country        0.0\n",
       "fromgoogle     0.0\n",
       "engnat         0.0\n",
       "age            0.0\n",
       "education      0.0\n",
       "gender         0.0\n",
       "orientation    0.0\n",
       "race           0.0\n",
       "religion       0.0\n",
       "hand           0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isnull().sum() * 100) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>testelapse</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.962715</td>\n",
       "      <td>3.829589</td>\n",
       "      <td>2.846558</td>\n",
       "      <td>3.186902</td>\n",
       "      <td>2.865440</td>\n",
       "      <td>3.672084</td>\n",
       "      <td>3.216539</td>\n",
       "      <td>3.184512</td>\n",
       "      <td>2.761233</td>\n",
       "      <td>3.522945</td>\n",
       "      <td>...</td>\n",
       "      <td>479.994503</td>\n",
       "      <td>1.576243</td>\n",
       "      <td>1.239962</td>\n",
       "      <td>30.370698</td>\n",
       "      <td>2.317878</td>\n",
       "      <td>1.654398</td>\n",
       "      <td>1.833413</td>\n",
       "      <td>5.013623</td>\n",
       "      <td>2.394359</td>\n",
       "      <td>1.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.360291</td>\n",
       "      <td>1.551683</td>\n",
       "      <td>1.664804</td>\n",
       "      <td>1.476879</td>\n",
       "      <td>1.545798</td>\n",
       "      <td>1.342238</td>\n",
       "      <td>1.490733</td>\n",
       "      <td>1.387382</td>\n",
       "      <td>1.511805</td>\n",
       "      <td>1.242890</td>\n",
       "      <td>...</td>\n",
       "      <td>3142.178542</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.440882</td>\n",
       "      <td>367.201726</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>0.640915</td>\n",
       "      <td>1.303454</td>\n",
       "      <td>1.970996</td>\n",
       "      <td>2.184164</td>\n",
       "      <td>0.495357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>324.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>119834.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23763.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Q1           Q2           Q3           Q4           Q5  \\\n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000   \n",
       "mean      1.962715     3.829589     2.846558     3.186902     2.865440   \n",
       "std       1.360291     1.551683     1.664804     1.476879     1.545798   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     3.000000     1.000000     2.000000     1.000000   \n",
       "50%       1.000000     5.000000     3.000000     3.000000     3.000000   \n",
       "75%       3.000000     5.000000     5.000000     5.000000     4.000000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "                Q6           Q7           Q8           Q9          Q10  ...  \\\n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  ...   \n",
       "mean      3.672084     3.216539     3.184512     2.761233     3.522945  ...   \n",
       "std       1.342238     1.490733     1.387382     1.511805     1.242890  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       3.000000     2.000000     2.000000     1.000000     3.000000  ...   \n",
       "50%       4.000000     3.000000     3.000000     3.000000     4.000000  ...   \n",
       "75%       5.000000     5.000000     4.000000     4.000000     5.000000  ...   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "          testelapse   fromgoogle       engnat           age    education  \\\n",
       "count    4184.000000  4184.000000  4184.000000   4184.000000  4184.000000   \n",
       "mean      479.994503     1.576243     1.239962     30.370698     2.317878   \n",
       "std      3142.178542     0.494212     0.440882    367.201726     0.874264   \n",
       "min         7.000000     1.000000     0.000000     13.000000     0.000000   \n",
       "25%       186.000000     1.000000     1.000000     18.000000     2.000000   \n",
       "50%       242.000000     2.000000     1.000000     21.000000     2.000000   \n",
       "75%       324.250000     2.000000     1.000000     27.000000     3.000000   \n",
       "max    119834.000000     2.000000     2.000000  23763.000000     4.000000   \n",
       "\n",
       "            gender  orientation         race     religion         hand  \n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  \n",
       "mean      1.654398     1.833413     5.013623     2.394359     1.190966  \n",
       "std       0.640915     1.303454     1.970996     2.184164     0.495357  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       1.000000     1.000000     5.000000     1.000000     1.000000  \n",
       "50%       2.000000     1.000000     6.000000     2.000000     1.000000  \n",
       "75%       2.000000     2.000000     6.000000     2.000000     1.000000  \n",
       "max       3.000000     5.000000     7.000000     7.000000     3.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "* `age` column may have typo error(s) because standard deviation and maximum value is more than 300. \n",
    "* Noted that `age` in source documentation is entered as text by participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>23763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age\n",
       "2075    123\n",
       "2137    409\n",
       "2690  23763"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['age']>100, ['age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Since there is no way to find the correct values for the age_\n",
    "_I will have to drop the rows_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([2075,2137,2690], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>testelapse</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "      <td>4181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.962688</td>\n",
       "      <td>3.829706</td>\n",
       "      <td>2.845013</td>\n",
       "      <td>3.187037</td>\n",
       "      <td>2.864865</td>\n",
       "      <td>3.672327</td>\n",
       "      <td>3.215260</td>\n",
       "      <td>3.184406</td>\n",
       "      <td>2.760344</td>\n",
       "      <td>3.523559</td>\n",
       "      <td>...</td>\n",
       "      <td>480.141593</td>\n",
       "      <td>1.575939</td>\n",
       "      <td>1.239895</td>\n",
       "      <td>24.581679</td>\n",
       "      <td>2.317867</td>\n",
       "      <td>1.654867</td>\n",
       "      <td>1.831858</td>\n",
       "      <td>5.012437</td>\n",
       "      <td>2.392490</td>\n",
       "      <td>1.190863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.360602</td>\n",
       "      <td>1.551874</td>\n",
       "      <td>1.664401</td>\n",
       "      <td>1.476915</td>\n",
       "      <td>1.545997</td>\n",
       "      <td>1.341916</td>\n",
       "      <td>1.490502</td>\n",
       "      <td>1.387127</td>\n",
       "      <td>1.511825</td>\n",
       "      <td>1.243060</td>\n",
       "      <td>...</td>\n",
       "      <td>3143.300111</td>\n",
       "      <td>0.494259</td>\n",
       "      <td>0.440852</td>\n",
       "      <td>10.869709</td>\n",
       "      <td>0.873939</td>\n",
       "      <td>0.640905</td>\n",
       "      <td>1.302078</td>\n",
       "      <td>1.971165</td>\n",
       "      <td>2.182517</td>\n",
       "      <td>0.494392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>119834.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Q1           Q2           Q3           Q4           Q5  \\\n",
       "count  4181.000000  4181.000000  4181.000000  4181.000000  4181.000000   \n",
       "mean      1.962688     3.829706     2.845013     3.187037     2.864865   \n",
       "std       1.360602     1.551874     1.664401     1.476915     1.545997   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     3.000000     1.000000     2.000000     1.000000   \n",
       "50%       1.000000     5.000000     3.000000     3.000000     3.000000   \n",
       "75%       3.000000     5.000000     5.000000     5.000000     4.000000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "                Q6           Q7           Q8           Q9          Q10  ...  \\\n",
       "count  4181.000000  4181.000000  4181.000000  4181.000000  4181.000000  ...   \n",
       "mean      3.672327     3.215260     3.184406     2.760344     3.523559  ...   \n",
       "std       1.341916     1.490502     1.387127     1.511825     1.243060  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       3.000000     2.000000     2.000000     1.000000     3.000000  ...   \n",
       "50%       4.000000     3.000000     3.000000     3.000000     4.000000  ...   \n",
       "75%       5.000000     5.000000     4.000000     4.000000     5.000000  ...   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "          testelapse   fromgoogle       engnat          age    education  \\\n",
       "count    4181.000000  4181.000000  4181.000000  4181.000000  4181.000000   \n",
       "mean      480.141593     1.575939     1.239895    24.581679     2.317867   \n",
       "std      3143.300111     0.494259     0.440852    10.869709     0.873939   \n",
       "min         7.000000     1.000000     0.000000    13.000000     0.000000   \n",
       "25%       186.000000     1.000000     1.000000    18.000000     2.000000   \n",
       "50%       242.000000     2.000000     1.000000    21.000000     2.000000   \n",
       "75%       324.000000     2.000000     1.000000    27.000000     3.000000   \n",
       "max    119834.000000     2.000000     2.000000    86.000000     4.000000   \n",
       "\n",
       "            gender  orientation         race     religion         hand  \n",
       "count  4181.000000  4181.000000  4181.000000  4181.000000  4181.000000  \n",
       "mean      1.654867     1.831858     5.012437     2.392490     1.190863  \n",
       "std       0.640905     1.302078     1.971165     2.182517     0.494392  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       1.000000     1.000000     5.000000     1.000000     1.000000  \n",
       "50%       2.000000     1.000000     6.000000     2.000000     1.000000  \n",
       "75%       2.000000     2.000000     6.000000     2.000000     1.000000  \n",
       "max       3.000000     5.000000     7.000000     7.000000     3.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check max and standard deviation\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The max and standard deviation value of `age` column has changed to `86` and `10.87` respectively_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>ctry_UA</th>\n",
       "      <th>ctry_US</th>\n",
       "      <th>ctry_UY</th>\n",
       "      <th>ctry_UZ</th>\n",
       "      <th>ctry_VE</th>\n",
       "      <th>ctry_VI</th>\n",
       "      <th>ctry_VN</th>\n",
       "      <th>ctry_ZA</th>\n",
       "      <th>ctry_ZM</th>\n",
       "      <th>ctry_ZW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  ctry_UA  ctry_US  ctry_UY  \\\n",
       "0   4   1   5   1   5   1   5   1   4    1  ...        0        1        0   \n",
       "1   1   5   1   4   2   5   5   4   1    5  ...        0        0        0   \n",
       "\n",
       "   ctry_UZ  ctry_VE  ctry_VI  ctry_VN  ctry_ZA  ctry_ZM  ctry_ZW  \n",
       "0        0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0        0  \n",
       "\n",
       "[2 rows x 149 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode `country` column\n",
    "df1 = pd.get_dummies(data=df, columns=['country'], prefix='ctry')\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Model the data.\n",
    "\n",
    "### 5. Suppose I wanted to use Q1 - Q44 to predict whether or not the person is left-handed. Would this be a classification or regression problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It would be a classification problem because the target variable only has 2 results, or at max, 3 results (yes for left-handed, no for right-handed, and inconclusive for ambidextrous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed based on their responses to Q1 - Q44. Before doing that, however, you remember that it is often a good idea to standardize your variables. In general, why would we standardize our variables? Give an example of when we would standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** when the variance and range of values between variables vary widely, standardisation is required to ensure each variable is weighed equally in the algorithm equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Give an example of when we might not standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** When the variables are categorical, then standardisation may not be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Based on your answers to 6 and 7, do you think we should standardize our predictor variables in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Based on my answers above, I would still standardise my variables because some of my variables do not share the same range of values as the rest, e.g demographic variables, and `age` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed. What munging/cleaning do we need to do to our $y$ variable in order to explicitly answer this question? Do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We will have to exclude the data points that are ambiguous for handedness, meaning values of `3` in the `hand` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3541\n",
       "2     452\n",
       "3     178\n",
       "0      10\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['hand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3993, 149)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1[(df1['hand']==1) | (df1['hand']==2)]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The professor for whom you work suggests that you set $k = 4$. In this specific case, why might this be a bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Because the target is to classify between left-handedness or not, `k=4` can result in situations where there is equal representation of both classifications within the `k=4`, and the classification may not work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let's *(finally)* use $k$-nearest neighbors to predict whether or not a person is left-handed!\n",
    "\n",
    "> Be sure to create a train/test split with your data!\n",
    "\n",
    "> Create four separate models, one with $k = 3$, one with $k = 5$, one with $k = 15$, and one with $k = 25$.\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn modules\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "'''\n",
    "dropped `introelapse` and `testelapse` as these variables unlikely linked to handedness, \n",
    "also the range of values in these variables might affect the KNN classifier\n",
    "'''\n",
    "X = df2.drop(columns=['hand','introelapse', 'testelapse'])\n",
    "y = df2['hand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_drop all the demographic columns. Isolate y to be just left handedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split with stratification of target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2832\n",
       "2     362\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check stratification\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    709\n",
       "2     90\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**StandardScaler**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "Z_train = ss.fit_transform(X_train)\n",
    "Z_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KNN_models:** $k = 3, 5, 15$ and $25$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Instantiate KNN model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_3 = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn_15 = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "knn_25 = KNeighborsClassifier(n_neighbors=25, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**What accuracy can be expected from the KNN model for this dataset and target variable?**_\n",
    "> Using cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_3 mean accuracy score: 0.851916144200627\n",
      "knn_5 mean accuracy score: 0.8681925940438872\n",
      "knn_15 mean accuracy score: 0.8863499216300941\n",
      "knn_25 mean accuracy score: 0.8866634012539185\n"
     ]
    }
   ],
   "source": [
    "# mean accuracy score\n",
    "print(f'knn_3 mean accuracy score: {cross_val_score(knn_3, Z_train, y_train, cv=10).mean()}') \n",
    "print(f'knn_5 mean accuracy score: {cross_val_score(knn_5, Z_train, y_train, cv=10).mean()}')\n",
    "print(f'knn_15 mean accuracy score: {cross_val_score(knn_15, Z_train, y_train, cv=10).mean()}')\n",
    "print(f'knn_25 mean accuracy score: {cross_val_score(knn_25, Z_train, y_train, cv=10).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Fit models**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_3 = knn_3.fit(Z_train, y_train)\n",
    "knn_5 = knn_5.fit(Z_train, y_train)\n",
    "knn_15 = knn_15.fit(Z_train, y_train)\n",
    "knn_25 = knn_25.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being good data scientists, we know that we might not run just one type of model. We might run many different models and see which is best.\n",
    "\n",
    "### 12. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, let's check the [documentation for logistic regression in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Is there default regularization? If so, what is it? If not, how do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes, there is default regularisation in logistic regression in sklearn. It is a `L2` penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, should we standardize our features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes, because I do not want the variables that have a wider range of values to influence its weight on the logistic regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Let's use logistic regression to predict whether or not the person is left-handed.\n",
    "\n",
    "\n",
    "> Be sure to use the same train/test split with your data as with your $k$-NN model above!\n",
    "\n",
    "> Create four separate models, one with LASSO and $\\alpha = 1$, one with LASSO and $\\alpha = 10$, one with Ridge and $\\alpha = 1$, and one with Ridge and $\\alpha = 10$. *(Hint: Be careful with how you specify $\\alpha$ in your model!)*\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# using the same X and y assignments for KNN models\n",
    "'''\n",
    "dropped `introelapse` and `testelapse` as these variables unlikely linked to handedness, \n",
    "also the range of values in these variables might affect the KNN classifier\n",
    "'''\n",
    "X = df2.drop(columns=['hand','introelapse', 'testelapse'])\n",
    "y = df2['hand']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# using the same train test split set used for KNN models\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**StandardScaler**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# using the same scaled train test set\n",
    "\n",
    "ss = StandardScaler()\n",
    "Z_train = ss.fit_transform(X_train)\n",
    "Z_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> LASSO (L1 penalty):  $\\alpha = 1$ and $\\alpha = 10$\n",
    "\n",
    "> Ridge (L2 penalty, default): $\\alpha = 1$ and $\\alpha = 10$\n",
    "\n",
    "_Selected `solver` = `liblinear` because it is one of the solver types that supports `L1` and `L2` regularisation within `sklearn.linear_model.LogisticRegression` module. Solver = `saga` gave a_ \n",
    "> convergence warning: The max_iter was reached which means the `coef_` did not converge\n",
    "\n",
    "_So `saga` was not used_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Instantiate Logistic regression models**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_l1_a1 = LogisticRegression(solver='liblinear', penalty='l1', C=1.0) # LASSO alpha = 1\n",
    "logr_l1_a10 = LogisticRegression(solver='liblinear', penalty='l1', C=10.0) # LASSO alpha = 10\n",
    "logr_l2_a1 = LogisticRegression(solver='liblinear', penalty='l2', C=1.0) # Ridge alpha = 1\n",
    "logr_l2_a10 = LogisticRegression(solver='liblinear', penalty='l2', C=10.0) # Ridge alpha = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Fit model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_l1_a1 = logr_l1_a1.fit(Z_train, y_train)\n",
    "logr_l1_a10 = logr_l1_a10.fit(Z_train, y_train)\n",
    "logr_l2_a1 = logr_l2_a1.fit(Z_train, y_train)\n",
    "logr_l2_a10 = logr_l2_a10.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the model(s).\n",
    "\n",
    "### 15. Before calculating any score on your data, take a step back. Think about your $X$ variable and your $Y$ variable. Do you think your $X$ variables will do a good job of predicting your $Y$ variable? Why or why not? What impact do you think this will have on your scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I do not think my $X$ variables will do a good job of predicting my $Y$ variable. Because the questions that contribute to $X$ variables values do not have an obvious or known relationship with any particular type of handedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Using accuracy as your metric, evaluate all eight of your models on both the training and testing sets. Put your scores below. (If you want to be fancy and generate a table in Markdown, there's a [Markdown table generator site linked here](https://www.tablesgenerator.com/markdown_tables#).)\n",
    "- Note: Your answers here might look a little weird. You didn't do anything wrong; that's to be expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Evaluate KNN models**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== knn=3 ================\n",
      "knn_3_train_score: 1.0\n",
      "knn_3_test_score: 0.8635794743429287\n",
      "\n",
      "=============== knn=5 ================\n",
      "knn_5_train_score: 1.0\n",
      "knn_5_test_score: 0.8823529411764706\n",
      "\n",
      "=============== knn=15 ===============\n",
      "knn_15_train_score: 1.0\n",
      "knn_15_test_score: 0.8886107634543179\n",
      "\n",
      "=============== knn=25 ===============\n",
      "knn_25_train_score: 1.0\n",
      "knn_25_test_score: 0.8873591989987485\n",
      "\n",
      "=============== l1_a1 ================\n",
      "logr_l1_a1_trn_score: 0.8872886662492173\n",
      "logr_l1_a1_tst_score: 0.8873591989987485\n",
      "\n",
      "=============== l1_a10 ===============\n",
      "logr_l1_a10_trn_score: 0.8869755792110207\n",
      "logr_l1_a10_tst_score: 0.8873591989987485\n",
      "\n",
      "=============== l2_a1 ================\n",
      "logr_l2_a1_trn_score: 0.8869755792110207\n",
      "logr_l2_a1_tst_score: 0.8873591989987485\n",
      "\n",
      "=============== l2_a10 ===============\n",
      "logr_l2_a10_trn_score: 0.8869755792110207\n",
      "logr_l2_a10_tst_score: 0.8873591989987485\n"
     ]
    }
   ],
   "source": [
    "# accuracy scores\n",
    "print(' knn=3 '.center(38, '='))\n",
    "print(f'knn_3_train_score: {knn_3.score(Z_train, y_train)}') \n",
    "print(f'knn_3_test_score: {knn_3.score(Z_test, y_test)}') \n",
    "print()\n",
    "print(' knn=5 '.center(38, '='))\n",
    "print(f'knn_5_train_score: {knn_5.score(Z_train, y_train)}') \n",
    "print(f'knn_5_test_score: {knn_5.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' knn=15 '.center(38, '='))\n",
    "print(f'knn_15_train_score: {knn_15.score(Z_train, y_train)}') \n",
    "print(f'knn_15_test_score: {knn_15.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' knn=25 '.center(38, '='))\n",
    "print(f'knn_25_train_score: {knn_25.score(Z_train, y_train)}') \n",
    "print(f'knn_25_test_score: {knn_25.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' l1_a1 '.center(38, '='))\n",
    "print(f'logr_l1_a1_trn_score: {logr_l1_a1.score(Z_train, y_train)}') \n",
    "print(f'logr_l1_a1_tst_score: {logr_l1_a1.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' l1_a10 '.center(38, '='))\n",
    "print(f'logr_l1_a10_trn_score: {logr_l1_a10.score(Z_train, y_train)}') \n",
    "print(f'logr_l1_a10_tst_score: {logr_l1_a10.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' l2_a1 '.center(38, '='))\n",
    "print(f'logr_l2_a1_trn_score: {logr_l2_a1.score(Z_train, y_train)}') \n",
    "print(f'logr_l2_a1_tst_score: {logr_l2_a1.score(Z_test, y_test)}')\n",
    "print()\n",
    "print(' l2_a10 '.center(38, '='))\n",
    "print(f'logr_l2_a10_trn_score: {logr_l2_a10.score(Z_train, y_train)}') \n",
    "print(f'logr_l2_a10_tst_score: {logr_l2_a10.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Shows overfitted KNN models on train data_\n",
    "\n",
    "> _But logistic regression models are performing almost equally for both training and test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_3_train_score = knn_3.score(Z_train, y_train) \n",
    "knn_3_test_score = knn_3.score(Z_test, y_test) \n",
    "\n",
    "knn_5_train_score = knn_5.score(Z_train, y_train) \n",
    "knn_5_test_score = knn_5.score(Z_test, y_test)\n",
    "\n",
    "knn_15_train_score = knn_15.score(Z_train, y_train) \n",
    "knn_15_test_score = knn_15.score(Z_test, y_test)\n",
    "\n",
    "knn_25_train_score = knn_25.score(Z_train, y_train) \n",
    "knn_25_test_score = knn_25.score(Z_test, y_test)\n",
    "\n",
    "logr_l1_a1_trn_score = logr_l1_a1.score(Z_train, y_train) \n",
    "logr_l1_a1_tst_score = logr_l1_a1.score(Z_test, y_test)\n",
    "\n",
    "logr_l1_a10_trn_score = logr_l1_a10.score(Z_train, y_train) \n",
    "logr_l1_a10_tst_score = logr_l1_a10.score(Z_test, y_test)\n",
    "\n",
    "logr_l2_a1_trn_score = logr_l2_a1.score(Z_train, y_train) \n",
    "logr_l2_a1_tst_score = logr_l2_a1.score(Z_test, y_test)\n",
    "\n",
    "logr_l2_a10_trn_score = logr_l2_a10.score(Z_train, y_train) \n",
    "logr_l2_a10_tst_score = logr_l2_a10.score(Z_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_3_accuracy_diff = abs((knn_3_train_score - knn_3_test_score)/ knn_3_train_score)*100\n",
    "knn_5_accuracy_diff = abs((knn_5_train_score - knn_5_test_score)/ knn_5_train_score)*100\n",
    "knn_15_accuracy_diff = abs((knn_15_train_score - knn_15_test_score)/ knn_15_train_score)*100\n",
    "knn_25_accuracy_diff = abs((knn_25_train_score - knn_25_test_score)/ knn_25_train_score)*100\n",
    "\n",
    "logr_l1_a1_accuracy_diff = abs((logr_l1_a1_trn_score - logr_l1_a1_tst_score)/ logr_l1_a1_trn_score)*100\n",
    "logr_l1_a10_accuracy_diff = abs((logr_l1_a10_trn_score - logr_l1_a10_tst_score)/ logr_l1_a10_trn_score)*100\n",
    "logr_l2_a1_accuracy_diff = abs((logr_l2_a1_trn_score - logr_l2_a1_tst_score)/ logr_l2_a1_trn_score)*100\n",
    "logr_l2_a10_accuracy_diff = abs((logr_l2_a10_trn_score - logr_l2_a10_tst_score)/ logr_l2_a10_trn_score)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== knn=3 ================\n",
      "accuracy perc diff: 13.64205256570713 %\n",
      "\n",
      "=============== knn=5 ================\n",
      "accuracy perc diff: 11.764705882352944 %\n",
      "\n",
      "=============== knn=15 ===============\n",
      "accuracy perc diff: 11.138923654568211 %\n",
      "\n",
      "=============== knn=25 ===============\n",
      "accuracy perc diff: 11.26408010012515 %\n",
      "\n",
      "=============== l1_a1 ================\n",
      "accuracy perc diff: 0.007949244954218821 %\n",
      "\n",
      "=============== l1_a10 ===============\n",
      "accuracy perc diff: 0.04325032128495186 %\n",
      "\n",
      "=============== l2_a1 ================\n",
      "accuracy perc diff: 0.04325032128495186 %\n",
      "\n",
      "=============== l2_a10 ===============\n",
      "accuracy perc diff: 0.04325032128495186 %\n"
     ]
    }
   ],
   "source": [
    "print(' knn=3 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {knn_3_accuracy_diff} %')\n",
    "print()\n",
    "print(' knn=5 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {knn_5_accuracy_diff} %')\n",
    "print()\n",
    "print(' knn=15 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {knn_15_accuracy_diff} %')\n",
    "print()\n",
    "print(' knn=25 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {knn_25_accuracy_diff} %')\n",
    "print()\n",
    "print(' l1_a1 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {logr_l1_a1_accuracy_diff} %')\n",
    "print()\n",
    "print(' l1_a10 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {logr_l1_a10_accuracy_diff} %')\n",
    "print()\n",
    "print(' l2_a1 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {logr_l2_a1_accuracy_diff} %')\n",
    "print()\n",
    "print(' l2_a10 '.center(38, '='))\n",
    "print(f'accuracy perc diff: {logr_l2_a10_accuracy_diff} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "|Model|Train score|Test score|Accuracy diff %|Comments|\n",
    "|---|--:|--:|--:|---|\n",
    "|knn=3|1.0|0.8635794743429287|13.64205256570713 %|Overfitted|\n",
    "|knn=5|1.0|0.8823529411764706|11.764705882352944 %|Overfitted|\n",
    "|knn=15|1.0|0.8886107634543179|11.138923654568211 %|Overfitted|\n",
    "|knn=25|1.0|0.8873591989987485|11.26408010012515 %|Overfitted|\n",
    "|L1, alpha=1|0.8872886662492173|0.8873591989987485|0.007949244954218821 %|Neither|\n",
    "|L1, alpha=10|0.8869755792110207|0.8873591989987485|0.04325032128495186 %|Neither|\n",
    "|L2, alpha=1|0.8869755792110207|0.8873591989987485|0.04325032128495186 %|Neither|\n",
    "|L2, alpha=10|0.8869755792110207|0.8873591989987485|0.04325032128495186 %|Neither|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. In which of your $k$-NN models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** There is evidence of overfitting for all my KNN models ($k$ = 3, 5, 15, 25). Because they all perform better on the training data than on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Broadly speaking, how does the value of $k$ in $k$-NN affect the bias-variance tradeoff? (i.e. As $k$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As $k$ increases, bias increases, variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. If you have a $k$-NN model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Answer:**__\n",
    "\n",
    "1. Reduce the number of features used in the model\n",
    "1. Apply regularisation to variables\n",
    "1. Use a different model\n",
    "1. Feed more data\n",
    "1. If using KNN, increase $k$-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. In which of your logistic regression models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** None of my logistic regression models show any evidence of overfitting. Because none of them perform better on training set than on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Broadly speaking, how does the value of $C$ in logistic regression affect the bias-variance tradeoff? (i.e. As $C$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As $C$ increases, bias decreases, variance increases. 1/C = alpha. alpha is penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. For your logistic regression models, play around with the regularization hyperparameter, $C$. As you vary $C$, what happens to the fit and coefficients in the model? What do you think this means in the context of this specific problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Instantiate Logistic regression models**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_l1_a0_5 = LogisticRegression(solver='liblinear', penalty='l1', C=0.5) # LASSO alpha = 0.5\n",
    "logr_l1_a1000 = LogisticRegression(solver='liblinear', penalty='l1', C=1000.0) # LASSO alpha = 1000\n",
    "logr_l2_a0_1 = LogisticRegression(solver='liblinear', penalty='l2', C=0.1) # Ridge alpha = 0.1\n",
    "logr_l2_a3000 = LogisticRegression(solver='liblinear', penalty='l2', C=3000.0) # Ridge alpha = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Fit model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_l1_a0_5 = logr_l1_a0_5.fit(Z_train, y_train)\n",
    "logr_l1_a1000 = logr_l1_a1000.fit(Z_train, y_train)\n",
    "logr_l2_a0_1 = logr_l2_a0_1.fit(Z_train, y_train)\n",
    "logr_l2_a3000 = logr_l2_a3000.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Models intercepts and coefficients**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== l1_C=0.5 ==============\n",
      "logr L1 C=0.5 Intercept: [-2.28859476]\n",
      "logr L1 C=0.5 Coefficients: [[-0.03700162 -0.00237809 -0.01237858 -0.02778704  0.11789611 -0.00986011\n",
      "   0.03396422 -0.18015198 -0.029699    0.07291112  0.03024217 -0.04295736\n",
      "  -0.05175165  0.          0.          0.1174592   0.0618089  -0.12400584\n",
      "  -0.10492399 -0.02996719 -0.11454725 -0.11956082 -0.13063521 -0.0258022\n",
      "   0.07299984  0.09134403  0.          0.0237365   0.08400723  0.02782061\n",
      "   0.          0.          0.01697291  0.01368972  0.0553251  -0.04046584\n",
      "  -0.03774368  0.03839922 -0.09508136 -0.09472259 -0.03414744 -0.09223491\n",
      "  -0.16394666 -0.03414725 -0.00860332 -0.15408153  0.          0.\n",
      "  -0.14691932  0.11129254 -0.03780446 -0.07493267 -0.01825939 -0.02429835\n",
      "   0.03724478 -0.04403871  0.00125653 -0.12546313 -0.01442547  0.07844697\n",
      "  -0.01750258 -0.02048096 -0.15727063  0.04222478  0.09839978 -0.08478701\n",
      "  -0.06957419  0.04032282 -0.01520401 -0.05732542  0.         -0.04400878\n",
      "  -0.01489304 -0.03915689 -0.01317405 -0.06327274 -0.10756644 -0.1231915\n",
      "  -0.02482361 -0.05829813  0.          0.01683849  0.17130543 -0.00362005\n",
      "   0.07695809 -0.16888346 -0.0220439   0.05503331 -0.14510381  0.06137783\n",
      "  -0.04873361  0.         -0.30503974 -0.00982884  0.02412617  0.\n",
      "  -0.06910185 -0.05360509 -0.0254584   0.         -0.08058232  0.09811566\n",
      "   0.05020109  0.         -0.03532655 -0.00279404 -0.00189501 -0.01146753\n",
      "  -0.08592761 -0.06337991 -0.02165437 -0.04585583  0.          0.\n",
      "   0.09679131  0.0469598  -0.00242735  0.08434544 -0.05643509  0.07585533\n",
      "  -0.0237241   0.03880556 -0.02067757 -0.08153728 -0.01803725 -0.07074936\n",
      "  -0.00696139  0.         -0.0646274  -0.03286262  0.         -0.05915633\n",
      "   0.10141018  0.06827094 -0.0060688  -0.05692358  0.11040575  0.26048949\n",
      "  -0.01254162 -0.00421512  0.05990682  0.         -0.00929589 -0.08971346\n",
      "  -0.02134207  0.06226137]]\n",
      "\n",
      "============= l1_C=1000 ==============\n",
      "logr L1 C=1000 Intercept: [-2.86106434]\n",
      "logr L1 C=1000 Coefficients: [[-5.25822228e-02 -8.31753252e-03 -1.77192817e-02 -3.69665824e-02\n",
      "   1.27008303e-01 -1.89885460e-02  4.49836997e-02 -2.05926506e-01\n",
      "  -3.50908027e-02  8.48343369e-02  4.05591949e-02 -5.47647265e-02\n",
      "  -6.09696425e-02  1.09581152e-02  8.22854159e-03  1.26683176e-01\n",
      "   7.15373629e-02 -1.31435760e-01 -1.14355915e-01 -3.74543443e-02\n",
      "  -1.39465929e-01 -1.33033147e-01 -1.38987097e-01 -3.48077553e-02\n",
      "   8.06188895e-02  1.03674382e-01  4.05750889e-02  3.67631012e-02\n",
      "   9.37413301e-02  4.13241932e-02 -9.72521603e-05  3.14922905e-03\n",
      "   2.37958532e-02  2.46266132e-02  6.96480271e-02 -4.68030015e-02\n",
      "  -4.74068066e-02  5.27254121e-02 -1.05719581e-01 -1.13010930e-01\n",
      "  -4.59517733e-02 -9.99444866e-02 -2.12314952e-01 -4.03559543e-02\n",
      "  -1.30721258e-02 -1.58079625e-01 -5.69232892e-03  5.65926273e-03\n",
      "  -1.57649046e-01  1.19544067e-01 -5.67310164e-02 -8.08744627e-02\n",
      "  -1.54534417e-01 -1.62920074e-01  3.54864001e-02 -3.06393816e-01\n",
      "  -2.16038820e-03 -1.81852753e-01 -1.50804648e-01  7.23246337e-02\n",
      "  -2.78216653e-01 -1.57833056e-01 -6.78649791e-01  1.76992163e-04\n",
      "   9.08455438e-02 -4.72252583e-01 -3.64566426e-01  3.47439519e-02\n",
      "  -1.50299843e-01 -3.51807391e-01 -2.90504634e-02 -3.09110335e-01\n",
      "  -2.05161904e-01 -1.75725498e-01 -1.49595114e-01 -4.05872028e-01\n",
      "  -5.15447836e-01 -5.97742967e-01 -2.14257704e-01 -9.88355652e-02\n",
      "   0.00000000e+00  7.23379919e-03  1.15109256e-01 -1.37173178e-01\n",
      "   7.05383213e-02 -6.76769827e-01 -2.50675321e-01  5.33444366e-02\n",
      "  -6.09925328e-01  5.07229015e-02 -2.79840220e-01  0.00000000e+00\n",
      "  -1.05044128e+00 -1.44958689e-01  1.55650418e-02  0.00000000e+00\n",
      "  -4.13752287e-01 -3.20165644e-01 -1.62221938e-01  0.00000000e+00\n",
      "  -4.25010696e-01  2.27809164e-01  4.52753315e-02  0.00000000e+00\n",
      "  -2.99585348e-01 -1.35737696e-01 -1.36095055e-01 -1.45385168e-01\n",
      "  -4.73126042e-01 -3.58266553e-01 -2.11795406e-01 -8.26554210e-02\n",
      "  -1.57523674e-02  0.00000000e+00  8.28340259e-02  4.32014135e-02\n",
      "  -1.37782016e-01  6.24246182e-02 -3.50581414e-01  5.73468479e-02\n",
      "  -2.13743758e-01  3.53717422e-02 -1.56359487e-01 -4.23981451e-01\n",
      "  -2.06794556e-01 -4.15509244e-01 -1.42536705e-01 -1.92488837e-02\n",
      "  -1.06405843e-01 -2.22909137e-01 -1.30290886e-01 -2.92561474e-01\n",
      "   2.25123116e-01  6.03095124e-02 -1.41311810e-01 -2.88476258e-01\n",
      "   2.39985319e-01  1.55344708e-01 -1.47110604e-01 -1.38962086e-01\n",
      "   5.69000376e-02  0.00000000e+00 -1.45461074e-01 -4.34454773e-01\n",
      "  -1.58662276e-01  5.80678952e-02]]\n",
      "\n",
      "============== l2_C=0.1 ==============\n",
      "logr L2 C=0.1 Intercept: [-2.22346492]\n",
      "logr L2 C=0.1 Coefficients: [[-4.50359860e-02 -7.59858939e-03 -1.67273540e-02 -3.43554718e-02\n",
      "   1.14276582e-01 -1.60438060e-02  3.91030196e-02 -1.78111258e-01\n",
      "  -3.34672239e-02  7.51727168e-02  3.51985091e-02 -4.94085262e-02\n",
      "  -5.45885848e-02  8.48335611e-03  6.61114740e-03  1.13196031e-01\n",
      "   6.51295912e-02 -1.19325252e-01 -1.03845017e-01 -3.58867578e-02\n",
      "  -1.16851743e-01 -1.20460994e-01 -1.25680967e-01 -3.09639446e-02\n",
      "   7.31653372e-02  9.35072748e-02  7.79780492e-03  3.25490512e-02\n",
      "   8.43673181e-02  3.63673573e-02  1.74980336e-03  1.16999715e-03\n",
      "   1.99176921e-02  2.09505927e-02  6.16264966e-02 -4.38359310e-02\n",
      "  -4.28611519e-02  4.73034004e-02 -9.48330963e-02 -9.94637103e-02\n",
      "  -4.16819614e-02 -9.09612644e-02 -1.65266527e-01 -3.64008221e-02\n",
      "  -1.35799858e-02 -1.47945573e-01  1.92316457e-04  2.14996470e-03\n",
      "  -1.41904590e-01  1.09108673e-01 -4.64150545e-02 -7.57384509e-02\n",
      "  -3.91865446e-02 -4.41722392e-02  3.63251068e-02 -6.30941655e-02\n",
      "   4.61494562e-03 -1.37897572e-01 -3.62498940e-02  7.32264385e-02\n",
      "  -4.38684961e-02 -4.11863354e-02 -1.45489620e-01  2.23352836e-02\n",
      "   9.11459167e-02 -9.57007211e-02 -8.52032502e-02  3.70469409e-02\n",
      "  -3.59027821e-02 -7.54517673e-02 -7.53476963e-03 -6.50272248e-02\n",
      "  -4.07524420e-02 -5.52119952e-02 -3.51611790e-02 -7.82820841e-02\n",
      "  -1.11877162e-01 -1.20436193e-01 -4.70378028e-02 -7.12374821e-02\n",
      "   0.00000000e+00  1.46847666e-02  1.35157398e-01 -2.65900553e-02\n",
      "   7.15419039e-02 -1.55254997e-01 -4.63625884e-02  5.34613960e-02\n",
      "  -1.40273831e-01  5.44706407e-02 -6.69922416e-02  0.00000000e+00\n",
      "  -2.42627557e-01 -3.18730773e-02  2.19777903e-02  0.00000000e+00\n",
      "  -8.43147274e-02 -7.25731789e-02 -4.46268167e-02  0.00000000e+00\n",
      "  -9.24197904e-02  1.12327438e-01  4.76261216e-02  0.00000000e+00\n",
      "  -5.74123206e-02 -2.58610395e-02 -2.59606184e-02 -3.25883968e-02\n",
      "  -9.60927219e-02 -8.04094642e-02 -4.51146570e-02 -6.01588853e-02\n",
      "  -6.61024003e-03  0.00000000e+00  8.60487219e-02  4.43750151e-02\n",
      "  -2.67325319e-02  7.13300878e-02 -7.49768473e-02  6.50958326e-02\n",
      "  -4.66345631e-02  3.69163743e-02 -4.04065928e-02 -9.25786727e-02\n",
      "  -4.17567590e-02 -8.56792144e-02 -3.02888422e-02 -9.96916606e-03\n",
      "  -7.96473773e-02 -5.36974032e-02 -2.15257040e-02 -7.64626332e-02\n",
      "   1.14556479e-01  6.29869563e-02 -2.95266184e-02 -7.38001778e-02\n",
      "   1.22356970e-01  1.96431467e-01 -3.43135568e-02 -2.76767773e-02\n",
      "   5.76885865e-02  0.00000000e+00 -3.13845458e-02 -1.00152499e-01\n",
      "  -4.21387947e-02  5.88806891e-02]]\n",
      "\n",
      "============= l2_C=3000 ==============\n",
      "logr L2 C=3000 Intercept: [-2.84230737]\n",
      "logr L2 C=3000 Coefficients: [[-5.25895819e-02 -8.32236361e-03 -1.77231044e-02 -3.69700787e-02\n",
      "   1.27015644e-01 -1.89949965e-02  4.49902342e-02 -2.05943670e-01\n",
      "  -3.50932856e-02  8.48404895e-02  4.05675444e-02 -5.47711170e-02\n",
      "  -6.09753596e-02  1.09703119e-02  8.22901593e-03  1.26689453e-01\n",
      "   7.15424933e-02 -1.31443742e-01 -1.14363721e-01 -3.74583619e-02\n",
      "  -1.39481590e-01 -1.33043197e-01 -1.38994462e-01 -3.48125194e-02\n",
      "   8.06247651e-02  1.03681956e-01  4.06120565e-02  3.67692972e-02\n",
      "   9.37491793e-02  4.13309670e-02 -1.05598505e-04  3.15489537e-03\n",
      "   2.38025412e-02  2.46336669e-02  6.96558901e-02 -4.68054750e-02\n",
      "  -4.74127118e-02  5.27326772e-02 -1.05725901e-01 -1.13026696e-01\n",
      "  -4.59586375e-02 -9.99516083e-02 -2.12356836e-01 -4.03615667e-02\n",
      "  -1.30742862e-02 -1.58080891e-01 -5.70435919e-03  5.66561519e-03\n",
      "  -1.57658655e-01  1.19552365e-01 -5.67398987e-02 -8.08799408e-02\n",
      "  -1.45276355e-01 -1.53659629e-01  8.19698468e-02 -2.70304804e-01\n",
      "   5.92636706e-02 -2.15070474e-02 -1.41588222e-01  1.15810474e-01\n",
      "  -2.42440610e-01 -1.48505299e-01 -5.78986856e-01  2.04218342e-01\n",
      "   1.40138835e-01 -4.07040133e-01 -3.21902591e-01  7.50110764e-02\n",
      "  -1.41001242e-01 -3.09141646e-01  1.39763448e-01 -2.73053844e-01\n",
      "  -1.85569123e-01 -1.66451398e-01 -1.40224233e-01 -3.50415072e-01\n",
      "  -4.45168425e-01 -5.09170690e-01 -1.94632664e-01 -6.25028152e-03\n",
      "   0.00000000e+00  8.58596784e-02  3.83934073e-01 -1.27983895e-01\n",
      "   1.17021135e-01 -5.80465342e-01 -2.22147683e-01  8.18329294e-02\n",
      "  -5.25736917e-01  1.20335436e-01 -2.51063510e-01  0.00000000e+00\n",
      "  -8.91786442e-01 -1.35756755e-01  9.24758890e-02  0.00000000e+00\n",
      "  -3.58888111e-01 -2.84184784e-01 -1.52741350e-01  0.00000000e+00\n",
      "  -3.69907653e-01  2.46928978e-01  6.85365764e-02  0.00000000e+00\n",
      "  -2.62664588e-01 -1.26556982e-01 -1.26884427e-01 -1.36203112e-01\n",
      "  -4.07950278e-01 -3.15588434e-01 -1.92147503e-01  2.46864879e-03\n",
      "   5.38496048e-02  0.00000000e+00  1.67970510e-01  7.60889609e-02\n",
      "  -1.28567178e-01  1.57852513e-01 -3.07930404e-01  1.77231690e-01\n",
      "  -1.93982286e-01  7.56399218e-02 -1.47052235e-01 -3.69082911e-01\n",
      "  -1.87169673e-01 -3.60363961e-01 -1.33257096e-01  4.43200293e-02\n",
      "  -3.01129484e-03 -2.02956785e-01 -1.21091991e-01 -2.63828891e-01\n",
      "   2.49739491e-01  1.06790883e-01 -1.32116090e-01 -2.60381338e-01\n",
      "   2.61341276e-01  6.12608239e-01 -1.37918174e-01 -1.29586339e-01\n",
      "   8.01604528e-02  0.00000000e+00 -1.35256554e-01 -3.79117634e-01\n",
      "  -1.49382000e-01  8.13309729e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(' l1_C=0.5 '.center(38, '='))\n",
    "print(f'logr L1 C=0.5 Intercept: {logr_l1_a0_5.intercept_}')\n",
    "print(f'logr L1 C=0.5 Coefficients: {logr_l1_a0_5.coef_}')\n",
    "print()\n",
    "print(' l1_C=1000 '.center(38, '='))\n",
    "print(f'logr L1 C=1000 Intercept: {logr_l1_a1000.intercept_}')\n",
    "print(f'logr L1 C=1000 Coefficients: {logr_l1_a1000.coef_}')\n",
    "print()\n",
    "print(' l2_C=0.1 '.center(38, '='))\n",
    "print(f'logr L2 C=0.1 Intercept: {logr_l2_a0_1.intercept_}')\n",
    "print(f'logr L2 C=0.1 Coefficients: {logr_l2_a0_1.coef_}')\n",
    "print()\n",
    "print(' l2_C=3000 '.center(38, '='))\n",
    "print(f'logr L2 C=3000 Intercept: {logr_l2_a3000.intercept_}')\n",
    "print(f'logr L2 C=3000 Coefficients: {logr_l2_a3000.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "* As I increase the $C$-value within `L1` and `L2` regularisation, the coefficients get smaller, the intercept also decreases.\n",
    "* It means that the regularisation is helping with overfitting of the logistic regression model, by reducing the weight of each variable with the increase in penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. If you have a logistic regression model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Answer:**__\n",
    "1. reduce number of features or increase data\n",
    "1. regularisation\n",
    "1. use a different solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Answer the problem.\n",
    "\n",
    "### 24. Suppose you want to understand which psychological features are most important in determining left-handedness. Would you rather use $k$-NN or logistic regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I would use logistic regression, because `L1` and `L2` regularisation helps bring some coefficients to zero, eliminating some variables from the equation, which guides me towards the other features that are more important in determining left-handedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Select your logistic regression model that utilized LASSO regularization with $\\alpha = 1$. Interpret the coefficient for `Q1`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# previous code above\n",
    "'''Instantiate model'''\n",
    "logr_l1_a1 = LogisticRegression(solver='liblinear', penalty='l1', C=1.0) # LASSO alpha = 1\n",
    "\n",
    "'''Fit model'''\n",
    "logr_l1_a1 = logr_l1_a1.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04477552, -0.00511478, -0.01479419, -0.03212176,  0.12256912,\n",
       "        -0.01411235,  0.03930677, -0.1926312 , -0.03263088,  0.07884156,\n",
       "         0.03532943, -0.04916223, -0.0562834 ,  0.00415261,  0.00344368,\n",
       "         0.12264699,  0.06693344, -0.1275371 , -0.10912277, -0.03363829,\n",
       "        -0.12656526, -0.12574779, -0.13408379, -0.03065176,  0.07730055,\n",
       "         0.09799066,  0.00194269,  0.03071261,  0.08883812,  0.03472107,\n",
       "         0.        ,  0.        ,  0.02053328,  0.01925867,  0.06234167,\n",
       "        -0.04400752, -0.0422888 ,  0.04537566, -0.10031398, -0.10308679,\n",
       "        -0.0395093 , -0.09609659, -0.17129444, -0.03744291, -0.01089062,\n",
       "        -0.15583946,  0.        ,  0.00045057, -0.15179668,  0.11573305,\n",
       "        -0.047046  , -0.07825271, -0.03121093, -0.03737941,  0.04192997,\n",
       "        -0.06781636,  0.00715816, -0.13421053, -0.02737186,  0.08058074,\n",
       "        -0.04039124, -0.03382575, -0.2088591 ,  0.04583077,  0.10035873,\n",
       "        -0.12220558, -0.0980446 ,  0.04260372, -0.027443  , -0.0854364 ,\n",
       "         0.00526954, -0.06914236, -0.0326577 , -0.05209561, -0.02578849,\n",
       "        -0.09477884, -0.14693498, -0.16868108, -0.04191747, -0.0670278 ,\n",
       "         0.        ,  0.0215397 ,  0.17576027, -0.01504967,  0.07927913,\n",
       "        -0.21924082, -0.04244208,  0.05741222, -0.19208888,  0.06447098,\n",
       "        -0.07031867,  0.        , -0.38346392, -0.02192029,  0.02918085,\n",
       "         0.        , -0.10194241, -0.07933054, -0.03839013,  0.        ,\n",
       "        -0.11326898,  0.11057787,  0.05077471,  0.        , -0.05946721,\n",
       "        -0.01412217, -0.01366224, -0.02318778, -0.12318873, -0.09162439,\n",
       "        -0.03945103, -0.05360275,  0.        ,  0.        ,  0.10009212,\n",
       "         0.04901168, -0.01481438,  0.08508992, -0.0844519 ,  0.08096429,\n",
       "        -0.04125854,  0.04197914, -0.033203  , -0.11342668, -0.03502383,\n",
       "        -0.10343361, -0.01934035,  0.        , -0.07284391, -0.05043356,\n",
       "        -0.00776869, -0.08188945,  0.11344575,  0.07003314, -0.01839827,\n",
       "        -0.07914327,  0.12323285,  0.26280007, -0.02477643, -0.01612593,\n",
       "         0.06152254,  0.        , -0.02158967, -0.12257821, -0.03467752,\n",
       "         0.06326155]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr_l1_a1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** LASSO logistic regression has helped to eliminate some variables from the equation by reducing the coefficient to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. If you have to select one model overall to be your *best* model, which model would you select? Why?\n",
    "- Usually in the \"real world,\" you'll fit many types of models but ultimately need to pick only one! (For example, a client may not understand what it means to have multiple models, or if you're using an algorithm to make a decision, it's probably pretty challenging to use two or more algorithms simultaneously.) It's not always an easy choice, but you'll have to make it soon enough. Pick a model and defend why you picked this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I would pick the `Logistic Regression` model with `L1` regularisation and $C$-value of `1`. Because it has the least accuracy difference between performance on training and test data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Circle back to the three specific and conclusively answerable questions you came up with in Q1. Answer one of these for the professor based on the model you selected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 146)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discovered it is 2D numpy array which code below cannot accept\n",
    "logr_l1_a1.coef_.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "or can use logr_l1_a1.coef_[0] instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame()\n",
    "sort_values(by='features', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to 1D numpy array\n",
    "flat_coef = logr_l1_a1.coef_.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04477552, -0.00511478, -0.01479419, -0.03212176,  0.12256912,\n",
       "       -0.01411235,  0.03930677, -0.1926312 , -0.03263088,  0.07884156,\n",
       "        0.03532943, -0.04916223, -0.0562834 ,  0.00415261,  0.00344368,\n",
       "        0.12264699,  0.06693344, -0.1275371 , -0.10912277, -0.03363829,\n",
       "       -0.12656526, -0.12574779, -0.13408379, -0.03065176,  0.07730055,\n",
       "        0.09799066,  0.00194269,  0.03071261,  0.08883812,  0.03472107,\n",
       "        0.        ,  0.        ,  0.02053328,  0.01925867,  0.06234167,\n",
       "       -0.04400752, -0.0422888 ,  0.04537566, -0.10031398, -0.10308679,\n",
       "       -0.0395093 , -0.09609659, -0.17129444, -0.03744291, -0.01089062,\n",
       "       -0.15583946,  0.        ,  0.00045057, -0.15179668,  0.11573305,\n",
       "       -0.047046  , -0.07825271, -0.03121093, -0.03737941,  0.04192997,\n",
       "       -0.06781636,  0.00715816, -0.13421053, -0.02737186,  0.08058074,\n",
       "       -0.04039124, -0.03382575, -0.2088591 ,  0.04583077,  0.10035873,\n",
       "       -0.12220558, -0.0980446 ,  0.04260372, -0.027443  , -0.0854364 ,\n",
       "        0.00526954, -0.06914236, -0.0326577 , -0.05209561, -0.02578849,\n",
       "       -0.09477884, -0.14693498, -0.16868108, -0.04191747, -0.0670278 ,\n",
       "        0.        ,  0.0215397 ,  0.17576027, -0.01504967,  0.07927913,\n",
       "       -0.21924082, -0.04244208,  0.05741222, -0.19208888,  0.06447098,\n",
       "       -0.07031867,  0.        , -0.38346392, -0.02192029,  0.02918085,\n",
       "        0.        , -0.10194241, -0.07933054, -0.03839013,  0.        ,\n",
       "       -0.11326898,  0.11057787,  0.05077471,  0.        , -0.05946721,\n",
       "       -0.01412217, -0.01366224, -0.02318778, -0.12318873, -0.09162439,\n",
       "       -0.03945103, -0.05360275,  0.        ,  0.        ,  0.10009212,\n",
       "        0.04901168, -0.01481438,  0.08508992, -0.0844519 ,  0.08096429,\n",
       "       -0.04125854,  0.04197914, -0.033203  , -0.11342668, -0.03502383,\n",
       "       -0.10343361, -0.01934035,  0.        , -0.07284391, -0.05043356,\n",
       "       -0.00776869, -0.08188945,  0.11344575,  0.07003314, -0.01839827,\n",
       "       -0.07914327,  0.12323285,  0.26280007, -0.02477643, -0.01612593,\n",
       "        0.06152254,  0.        , -0.02158967, -0.12257821, -0.03467752,\n",
       "        0.06326155])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check conversion\n",
    "flat_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26280007,  0.17576027,  0.12323285,  0.12264699,  0.12256912,\n",
       "        0.11573305,  0.11344575,  0.11057787,  0.10035873,  0.10009212,\n",
       "        0.09799066,  0.08883812,  0.08508992,  0.08096429,  0.08058074,\n",
       "        0.07927913,  0.07884156,  0.07730055,  0.07003314,  0.06693344,\n",
       "        0.06447098,  0.06326155,  0.06234167,  0.06152254,  0.05741222,\n",
       "        0.05077471,  0.04901168,  0.04583077,  0.04537566,  0.04260372,\n",
       "        0.04197914,  0.04192997,  0.03930677,  0.03532943,  0.03472107,\n",
       "        0.03071261,  0.02918085,  0.0215397 ,  0.02053328,  0.01925867,\n",
       "        0.00715816,  0.00526954,  0.00415261,  0.00344368,  0.00194269,\n",
       "        0.00045057,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.00511478, -0.00776869,\n",
       "       -0.01089062, -0.01366224, -0.01411235, -0.01412217, -0.01479419,\n",
       "       -0.01481438, -0.01504967, -0.01612593, -0.01839827, -0.01934035,\n",
       "       -0.02158967, -0.02192029, -0.02318778, -0.02477643, -0.02578849,\n",
       "       -0.02737186, -0.027443  , -0.03065176, -0.03121093, -0.03212176,\n",
       "       -0.03263088, -0.0326577 , -0.033203  , -0.03363829, -0.03382575,\n",
       "       -0.03467752, -0.03502383, -0.03737941, -0.03744291, -0.03839013,\n",
       "       -0.03945103, -0.0395093 , -0.04039124, -0.04125854, -0.04191747,\n",
       "       -0.0422888 , -0.04244208, -0.04400752, -0.04477552, -0.047046  ,\n",
       "       -0.04916223, -0.05043356, -0.05209561, -0.05360275, -0.0562834 ,\n",
       "       -0.05946721, -0.0670278 , -0.06781636, -0.06914236, -0.07031867,\n",
       "       -0.07284391, -0.07825271, -0.07914327, -0.07933054, -0.08188945,\n",
       "       -0.0844519 , -0.0854364 , -0.09162439, -0.09477884, -0.09609659,\n",
       "       -0.0980446 , -0.10031398, -0.10194241, -0.10308679, -0.10343361,\n",
       "       -0.10912277, -0.11326898, -0.11342668, -0.12220558, -0.12257821,\n",
       "       -0.12318873, -0.12574779, -0.12656526, -0.1275371 , -0.13408379,\n",
       "       -0.13421053, -0.14693498, -0.15179668, -0.15583946, -0.16868108,\n",
       "       -0.17129444, -0.19208888, -0.1926312 , -0.2088591 , -0.21924082,\n",
       "       -0.38346392])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the coefficients to find which variables have the highest weights\n",
    "np.flip(np.sort(flat_coef)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Higher the score for a question, higher the agreement to the statement\n",
    "\n",
    "> For `hand` (target), `1`=`Right`, `2`=`Left`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Only positive coefficients would signify a direct relationship between an affirmed personality trait and left-handedness because of how the scoring is done (`5` = `agree` and `2` = `left`)_ <br><br>\n",
    "_After reviewing the top positive coefficients in descending order, and excluding those that relate to demographics (since they are not personality traits), the strongest relationships are found in `Q5`, `Q16`, and `Q26`._ <br><br>\n",
    "_So the following questions are formulated based on the strongest relationships. The questions above are also revised to match these questions below_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> Is a person who day-dreams, more likely to be left-handed? <br>\n",
    "    _Q5_ \n",
    "    \n",
    "> Is a person who puts on fake concerts as a child, more likely to be left-handed? <br>\n",
    "    _Q16_\n",
    "    \n",
    "> Is a person who jumps when excited, more likely to be left-handed? <br>\n",
    "    _Q26_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5\n",
      "Q16\n",
      "Q26\n"
     ]
    }
   ],
   "source": [
    "# to confirm the coef array index match which questions\n",
    "print(np.array(X.columns)[4])\n",
    "print(np.array(X.columns)[15])\n",
    "print(np.array(X.columns)[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Daydream ====================\n",
      " Q5. I have day dreamed about saving someone from a burning building: 0.1225691222017266\n",
      "\n",
      "================= Fake concerts ==================\n",
      " Q16. When I was a child, I put on fake concerts and plays with my friends: 0.12264698918927933\n",
      "\n",
      "=============== Jump in excitement ===============\n",
      " Q26. I jump up and down in excitement sometimes: 0.09799066437033165\n"
     ]
    }
   ],
   "source": [
    "print(' Daydream '.center(50, '='))\n",
    "print(f' Q5. I have day dreamed about saving someone from a burning building: {flat_coef[4]}')\n",
    "print()\n",
    "print(' Fake concerts '.center(50, '='))\n",
    "print(f' Q16. When I was a child, I put on fake concerts and plays with my friends: {flat_coef[15]}')\n",
    "print()\n",
    "print(' Jump in excitement '.center(50, '='))\n",
    "print(f' Q26. I jump up and down in excitement sometimes: {flat_coef[25]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Answer:**__ \n",
    "1. A personality trait of daydreaming about saving someone from a burning building is linked to a higher likelihood of being left-handed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS:\n",
    "Looking for more to do? Probably not - you're busy! But if you want to, consider exploring the following. (They could make for a blog post!)\n",
    "- Create a visual plot comparing training and test metrics for various values of $k$ and various regularization schemes in logistic regression.\n",
    "- Rather than just evaluating models based on accuracy, consider using sensitivity, specificity, etc.\n",
    "- In the context of predicting left-handedness, why are unbalanced classes concerning? If you were to re-do this process given those concerns, what changes might you make?\n",
    "- Fit and evaluate a generalized linear model other than logistic regression (e.g. Poisson regression).\n",
    "- Suppose this data were in a `SQL` database named `data` and a table named `inventory`. What `SQL` query would return the count of people who were right-handed, left-handed, both, or missing with their class labels of 1, 2, 3, and 0, respectively? (You can assume you've already logged into the database.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
