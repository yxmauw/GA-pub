{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./401ksubs.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9275 entries, 0 to 9274\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   e401k   9275 non-null   int64  \n",
      " 1   inc     9275 non-null   float64\n",
      " 2   marr    9275 non-null   int64  \n",
      " 3   male    9275 non-null   int64  \n",
      " 4   age     9275 non-null   int64  \n",
      " 5   fsize   9275 non-null   int64  \n",
      " 6   nettfa  9275 non-null   float64\n",
      " 7   p401k   9275 non-null   int64  \n",
      " 8   pira    9275 non-null   int64  \n",
      " 9   incsq   9275 non-null   float64\n",
      " 10  agesq   9275 non-null   int64  \n",
      "dtypes: float64(3), int64(8)\n",
      "memory usage: 797.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.392129</td>\n",
       "      <td>39.254641</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.204420</td>\n",
       "      <td>41.080216</td>\n",
       "      <td>2.885067</td>\n",
       "      <td>19.071675</td>\n",
       "      <td>0.276226</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>2121.192483</td>\n",
       "      <td>1793.652722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.488252</td>\n",
       "      <td>24.090002</td>\n",
       "      <td>0.483213</td>\n",
       "      <td>0.403299</td>\n",
       "      <td>10.299517</td>\n",
       "      <td>1.525835</td>\n",
       "      <td>63.963838</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.435513</td>\n",
       "      <td>3001.469424</td>\n",
       "      <td>895.648841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-502.302000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.160100</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>469.155600</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.288000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1108.091000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.160000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.449500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2516.025500</td>\n",
       "      <td>2304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>199.041000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1536.798000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39617.320000</td>\n",
       "      <td>4096.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             e401k          inc         marr         male          age  \\\n",
       "count  9275.000000  9275.000000  9275.000000  9275.000000  9275.000000   \n",
       "mean      0.392129    39.254641     0.628571     0.204420    41.080216   \n",
       "std       0.488252    24.090002     0.483213     0.403299    10.299517   \n",
       "min       0.000000    10.008000     0.000000     0.000000    25.000000   \n",
       "25%       0.000000    21.660000     0.000000     0.000000    33.000000   \n",
       "50%       0.000000    33.288000     1.000000     0.000000    40.000000   \n",
       "75%       1.000000    50.160000     1.000000     0.000000    48.000000   \n",
       "max       1.000000   199.041000     1.000000     1.000000    64.000000   \n",
       "\n",
       "             fsize       nettfa        p401k         pira         incsq  \\\n",
       "count  9275.000000  9275.000000  9275.000000  9275.000000   9275.000000   \n",
       "mean      2.885067    19.071675     0.276226     0.254340   2121.192483   \n",
       "std       1.525835    63.963838     0.447154     0.435513   3001.469424   \n",
       "min       1.000000  -502.302000     0.000000     0.000000    100.160100   \n",
       "25%       2.000000    -0.500000     0.000000     0.000000    469.155600   \n",
       "50%       3.000000     2.000000     0.000000     0.000000   1108.091000   \n",
       "75%       4.000000    18.449500     1.000000     1.000000   2516.025500   \n",
       "max      13.000000  1536.798000     1.000000     1.000000  39617.320000   \n",
       "\n",
       "             agesq  \n",
       "count  9275.000000  \n",
       "mean   1793.652722  \n",
       "std     895.648841  \n",
       "min     625.000000  \n",
       "25%    1089.000000  \n",
       "50%    1600.000000  \n",
       "75%    2304.000000  \n",
       "max    4096.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Other variables that may influence customers to purchase a new investment product are:\n",
    "* disposible income \n",
    "* already have other investment products \n",
    "* property value\n",
    "* shopping habits (saver or spender), \n",
    "* whether have monthly saving habit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "Because that would be discriminating based on race, which is a sensitive issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Features such as `e401k`, `p401k`, `pira` would not be used because they are outcomes of having an income, would not be realistic contributing factors to **income**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# multicollinearity check\n",
    "\n",
    "plt.figure(figsize = (6,2.5))\n",
    "\n",
    "# Get correlation of variables.\n",
    "corr_df = df.drop(columns=['e401k','p401k','pira','incsq','inc','agesq']).corr()\n",
    "corr = corr_df[abs(corr_df)>=.5] # show only correlations greater than .5\n",
    "\n",
    "# Set up mask to be \"True\" in the upper triangle.\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(corr, \n",
    "            cmap='Blues', \n",
    "            mask=mask, \n",
    "            annot = True,\n",
    "            vmin=0\n",
    "           )\n",
    "plt.title('Correlation Heatmap of 401(k) Dataset', fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inc       1.000000\n",
       "nettfa    0.376586\n",
       "marr      0.362008\n",
       "fsize     0.110170\n",
       "age       0.105638\n",
       "male     -0.069871\n",
       "Name: inc, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat = df.drop(columns=['e401k','p401k','pira','incsq','agesq']).corr().inc.sort_values(ascending=False)\n",
    "corr_mat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no strong correlating factors that predicts income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "The 2 variables that have been created through feature engineering are polynomial features of `income` and `age`. \n",
    "\n",
    "Subject-matter experts on identifying potential customers for investment products might have believed that income and age are highly correlated with interest and buying potential of investment products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "There are 2 variable descriptions that appear to have errors. They are `inc` and `age`. They share the same labels as their polynomial of 2 degree counterparts. \n",
    "\n",
    "Correct values should be `income` and `age` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Regression models that I have learnt are:\n",
    "1. Linear models - Linear regression, Ridge regression, Lasso regression, ElasticNet regression\n",
    "<details>This is appropriate for our problem because the coefficients can be elicited and the predictions are continuous values </details>\n",
    "1. Random Forest regressor \n",
    "<details>This is not appropriate because the coefficients are not easy to understand.</details>\n",
    "1. Generalised linear models e.g Poisson regression, Gamma regression\n",
    "<details>This is not appropriate because for Poisson, it predicts discrete $Y$ variable, while Gamma is more appropriate for waiting time predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression: What features best predict one's income?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = df.drop(columns=['e401k','p401k','pira','inc','incsq'])\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.3, #9275 obs\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6492, 6)\n",
      "y_train shape: (6492,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:' , X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate pipeline to test multiple models\n",
    "# built-in min max scaler\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler # chosen for feature interaction explainability\n",
    "\n",
    "# import models\n",
    "from sklearn.linear_model import ElasticNet # no CV because will be done by GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor # for bagging decision trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor # to use in adaboost\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Pipelines and GridSearchCV\n",
    "[GridSearch + Pipeline multiple models reference](https://ryan-reilly.medium.com/gridsearch-pipelines-of-multiple-models-on-multiclass-classification-e9124b6ea2e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipelines\n",
    "enet_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('enet', ElasticNet())\n",
    "])\n",
    "\n",
    "knr_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('knr', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "dt_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('dtr', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "bag_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('bag', BaggingRegressor()) # check if default is decision trees\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('rf', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "abr_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('abr', AdaBoostRegressor())\n",
    "])\n",
    "\n",
    "svr_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('svr', SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "seeds=[1,2,3]\n",
    "\n",
    "enet_params = [{\n",
    "    'enet__alpha':[.5,.7,1.],\n",
    "    'enet__l1_ratio':[.5,.7,.9]\n",
    "}]\n",
    "\n",
    "knr_params = [{\n",
    "    'knr__n_neighbors':[3,5,9],\n",
    "    'knr__weights':['uniform','distance']\n",
    "}]\n",
    "\n",
    "dt_params = [{\n",
    "    'dtr__random_state':seeds,\n",
    "    'dtr__splitter':['random','best'],\n",
    "    'dtr__max_depth':[None,3,5]\n",
    "}]\n",
    "\n",
    "bag_params = [{\n",
    "    'bag__random_state':seeds,\n",
    "    'bag__n_estimators':[5,10]\n",
    "}]\n",
    "\n",
    "rf_params = [{\n",
    "    'rf__random_state':seeds,\n",
    "    'rf__n_estimators':[100,150,200],\n",
    "    'rf__max_depth':[None,3,5]\n",
    "}]\n",
    "\n",
    "abr_params = [{\n",
    "    'abr__base_estimator':[SGDRegressor()],\n",
    "    'abr__learning_rate':[.5,1.],\n",
    "    'abr__random_state':seeds,\n",
    "    'abr__loss':['linear','square','exponential']\n",
    "}]\n",
    "\n",
    "svr_params = [{\n",
    "    'svr__C':[1.,10.,100.],\n",
    "    'svr__max_iter':[20_000,40_000]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_gs = GridSearchCV(enet_pipe,\n",
    "                       enet_params,\n",
    "                       scoring='neg_root_mean_squared_error',\n",
    "                       cv=5,\n",
    "                       verbose=1)\n",
    "\n",
    "knr_gs = GridSearchCV(knr_pipe,\n",
    "                      knr_params,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "dt_gs = GridSearchCV(dt_pipe,\n",
    "                     dt_params,\n",
    "                     scoring='neg_root_mean_squared_error',\n",
    "                     cv=5,\n",
    "                     verbose=1)\n",
    "\n",
    "bag_gs = GridSearchCV(bag_pipe,\n",
    "                      bag_params,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "rf_gs = GridSearchCV(rf_pipe,\n",
    "                     rf_params,\n",
    "                     scoring='neg_root_mean_squared_error',\n",
    "                     cv=5,\n",
    "                     verbose=1)\n",
    "\n",
    "abr_gs = GridSearchCV(abr_pipe,\n",
    "                      abr_params,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "svr_gs = GridSearchCV(svr_pipe,\n",
    "                      svr_params,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                      cv=5,\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 02/08/2022, 14:57\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "CPU times: total: 17min 45s\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "named_tuple = time.localtime() # get start_time\n",
    "time_string = time.strftime(\"%d/%m/%Y, %H:%M\", named_tuple) # format time print\n",
    "print(f'start time: {time_string}')\n",
    "\n",
    "# fit models\n",
    "grids = [enet_gs, knr_gs, dt_gs, bag_gs, rf_gs, abr_gs, svr_gs]\n",
    "for pipe in grids:\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_bar rmse: 23.87504286627291\n",
      "Elastic Net train rmse: -22.30602858487258\n",
      "Elastic Net test rmse: -23.08709162981089\n",
      "Elastic Net %_diff rmse: -3.501578248079575 %\n",
      "Elastic Net Best Params: {'enet__alpha': 0.5, 'enet__l1_ratio': 0.9}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "K-Neighbors train rmse: -17.976878251026275\n",
      "K-Neighbors test rmse: -20.70924699598627\n",
      "K-Neighbors %_diff rmse: -15.199350559121733 %\n",
      "K-Neighbors Best Params: {'knr__n_neighbors': 9, 'knr__weights': 'uniform'}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Decision Tree train rmse: -18.12669591446635\n",
      "Decision Tree test rmse: -19.428925746939772\n",
      "Decision Tree %_diff rmse: -7.184044122647599 %\n",
      "Decision Tree Best Params: {'dtr__max_depth': 5, 'dtr__random_state': 1, 'dtr__splitter': 'best'}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Bagged Decision Trees train rmse: -8.77753858636459\n",
      "Bagged Decision Trees test rmse: -21.079412501068443\n",
      "Bagged Decision Trees %_diff rmse: -140.15174976062332 %\n",
      "Bagged Decision Trees Best Params: {'bag__n_estimators': 10, 'bag__random_state': 2}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Random Forest train rmse: -17.923709661216055\n",
      "Random Forest test rmse: -19.17695530000992\n",
      "Random Forest %_diff rmse: -6.992110798947386 %\n",
      "Random Forest Best Params: {'rf__max_depth': 5, 'rf__n_estimators': 100, 'rf__random_state': 2}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "AdaBoost train rmse: -20.379655479386884\n",
      "AdaBoost test rmse: -21.394838350923177\n",
      "AdaBoost %_diff rmse: -4.981354432429466 %\n",
      "AdaBoost Best Params: {'abr__base_estimator': SGDRegressor(), 'abr__learning_rate': 1.0, 'abr__loss': 'linear', 'abr__random_state': 1}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Support Vector regressor train rmse: -19.689750483393574\n",
      "Support Vector regressor test rmse: -20.773610443075317\n",
      "Support Vector regressor %_diff rmse: -5.504691187406742 %\n",
      "Support Vector regressor Best Params: {'svr__C': 100.0, 'svr__max_iter': 20000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_bar = np.repeat(np.mean(y_train),(len(y_train)))\n",
    "y_bar_rmse = mean_squared_error(y_train, y_bar, squared=False)\n",
    "\n",
    "grid_dict = {0: 'Elastic Net', 1: 'K-Neighbors',\n",
    "            2: 'Decision Tree', 3: 'Bagged Decision Trees',\n",
    "            4: 'Random Forest', 5: 'AdaBoost',\n",
    "            6: 'Support Vector regressor'}\n",
    "\n",
    "for i, model in enumerate(grids):\n",
    "    print(f'y_bar rmse: {y_bar_rmse}')\n",
    "    print(f'{grid_dict[i]} train rmse: {model.score(X_train, y_train)}')\n",
    "    print(f'{grid_dict[i]} test rmse: {model.score(X_test, y_test)}')\n",
    "    print(f'{grid_dict[i]} %_diff rmse: {((model.score(X_train, y_train)-model.score(X_test, y_test))*100)/model.score(X_train, y_train)} %')\n",
    "    print(f'{grid_dict[i]} Best Params: {model.best_params_}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of my models currently have poor performance, not significantly better than baseline model. The best performing one (`Random Forest regressor`) is overfitted. I would need to do more hyperparameter tuning, maybe reduce features to reduce noise, do polynomial feature engineering of the more stronger correlating features such as `marr` or `nettfa`, maybe get data of other features not yet considered in this dataset.\n",
    "\n",
    "Based on `Random Forest regressor`, I will attempt to distill the predictive features for one's income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipeline object attribute distillation](https://stackoverflow.com/questions/69945795/scikit-learn-gridsearchcv-with-pipeline-as-estimator-how-to-get-the-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2303284 , 0.0068539 , 0.0231272 , 0.00379621, 0.71184085,\n",
       "       0.02405346])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = rf_gs.best_estimator_.named_steps['rf'].feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_import = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh(title, y, x, dec, limit):  \n",
    "    fig, ax = plt.subplots() \n",
    "    plt.title(f'{title} Feature Importance', fontsize=13) # title\n",
    "\n",
    "    hbars = plt.barh(y=y[::-1], # to arrange highest to lowest\n",
    "                 width=x[::-1], # to arrange highest to lowest\n",
    "                 align='center')\n",
    "\n",
    "    ax.bar_label(hbars, fmt=dec, padding=2) # annotate each horizontal bar as subplot\n",
    "    plt.xlim([0, limit]) # expand figure to fit annotations\n",
    "    plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "barh(title='Random Forest classifier', \n",
    "     y=X_train.columns[order_of_import],\n",
    "     x=importances[order_of_import],\n",
    "     dec='%.4f',\n",
    "     limit=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Random Forest Classifier` also agrees that `nettfa` and `marr` are the 2 most important features for predicting income. `nettfa` more so than `marr`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.\n",
    "\n",
    "[Reference](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "decision tree is a single machine learning estimator\n",
    "\n",
    "while bagged decision trees means bootstrapping the dataset to generate multiple random subsets, and fitting a decision tree for each subset to generate models of variable performance. then using averaging or voting of each model predictions to give final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Bagging is an ensemble algorithm that fits multiple models on different subsets of a training dataset, then combines the predictions from all models.\n",
    "\n",
    "Random forest is an extension of bagging that also randomly selects **subsets of features** used in each data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Random forests are usually superior to bagged decision trees, as, not only is bagging occurring, but random selection of a subset of features at every node is occurring, and, in practice, this reduces the correlation between trees, which improves the effectiveness of the final averaging step.\n",
    "\n",
    "[Reference](https://stats.stackexchange.com/questions/365437/what-are-advantages-of-random-forests-vs-using-bagging-with-other-classifiers#:~:text=Random%20forests%20are%20actually%20usually,of%20the%20final%20averaging%20step.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_bar rmse: 23.87504286627291\n",
      "Elastic Net train rmse: -22.30602858487258\n",
      "Elastic Net test rmse: -23.08709162981089\n",
      "Elastic Net %_diff rmse: -3.501578248079575 %\n",
      "Elastic Net Best Params: {'enet__alpha': 0.5, 'enet__l1_ratio': 0.9}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "K-Neighbors train rmse: -17.976878251026275\n",
      "K-Neighbors test rmse: -20.70924699598627\n",
      "K-Neighbors %_diff rmse: -15.199350559121733 %\n",
      "K-Neighbors Best Params: {'knr__n_neighbors': 9, 'knr__weights': 'uniform'}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Decision Tree train rmse: -18.12669591446635\n",
      "Decision Tree test rmse: -19.428925746939772\n",
      "Decision Tree %_diff rmse: -7.184044122647599 %\n",
      "Decision Tree Best Params: {'dtr__max_depth': 5, 'dtr__random_state': 1, 'dtr__splitter': 'best'}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Bagged Decision Trees train rmse: -8.77753858636459\n",
      "Bagged Decision Trees test rmse: -21.079412501068443\n",
      "Bagged Decision Trees %_diff rmse: -140.15174976062332 %\n",
      "Bagged Decision Trees Best Params: {'bag__n_estimators': 10, 'bag__random_state': 2}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Random Forest train rmse: -17.923709661216055\n",
      "Random Forest test rmse: -19.17695530000992\n",
      "Random Forest %_diff rmse: -6.992110798947386 %\n",
      "Random Forest Best Params: {'rf__max_depth': 5, 'rf__n_estimators': 100, 'rf__random_state': 2}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "AdaBoost train rmse: -20.379655479386884\n",
      "AdaBoost test rmse: -21.394838350923177\n",
      "AdaBoost %_diff rmse: -4.981354432429466 %\n",
      "AdaBoost Best Params: {'abr__base_estimator': SGDRegressor(), 'abr__learning_rate': 1.0, 'abr__loss': 'linear', 'abr__random_state': 1}\n",
      "\n",
      "y_bar rmse: 23.87504286627291\n",
      "Support Vector regressor train rmse: -19.689750483393574\n",
      "Support Vector regressor test rmse: -20.773610443075317\n",
      "Support Vector regressor %_diff rmse: -5.504691187406742 %\n",
      "Support Vector regressor Best Params: {'svr__C': 100.0, 'svr__max_iter': 20000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_bar = np.repeat(np.mean(y_train),(len(y_train)))\n",
    "y_bar_rmse = mean_squared_error(y_train, y_bar, squared=False)\n",
    "\n",
    "grid_dict = {0: 'Elastic Net', 1: 'K-Neighbors',\n",
    "            2: 'Decision Tree', 3: 'Bagged Decision Trees',\n",
    "            4: 'Random Forest', 5: 'AdaBoost',\n",
    "            6: 'Support Vector regressor'}\n",
    "\n",
    "for i, model in enumerate(grids):\n",
    "    print(f'y_bar rmse: {y_bar_rmse}')\n",
    "    print(f'{grid_dict[i]} train rmse: {model.score(X_train, y_train)}')\n",
    "    print(f'{grid_dict[i]} test rmse: {model.score(X_test, y_test)}')\n",
    "    print(f'{grid_dict[i]} %_diff rmse: {((model.score(X_train, y_train)-model.score(X_test, y_test))*100)/model.score(X_train, y_train)} %')\n",
    "    print(f'{grid_dict[i]} Best Params: {model.best_params_}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of my models currently have poor performance, not significantly better than baseline model. The best performing one (`Random Forest regressor`) is overfitted. I would need to do more hyperparameter tuning, maybe reduce features to reduce noise, do polynomial feature engineering of the more stronger correlating features such as `marr` or `nettfa`, maybe get data of other features not yet considered in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answers:\n",
    "\n",
    "There is overfitting in `K-Neighbors`, `Decision Tree`, `Bagged Decision Trees`, `Random Forest` and `SVR` models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I picked `Random Forest` despite overfitting because it has the best test RMSE score out of the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I would need to do more hyperparameter tuning, maybe reduce features to reduce noise, do polynomial feature engineering of the more stronger correlating features such as `marr` or `nettfa`, maybe get data of other features not yet considered in this dataset. Try other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "Not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "1. Logistic regression\n",
    "1. Naive Bayes \n",
    "1. K-Nearest Neighbors\n",
    "1. Decision trees\n",
    "1. Random Forest\n",
    "1. Support Vector machines\n",
    "\n",
    "Unable to identify why any of the techniques above is not suitable for solving this specific classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = df.drop(columns=['e401k','p401k','agesq','incsq'])\n",
    "y = df['e401k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.3, #9275 obs\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6492, 7)\n",
      "y_train shape: (6492,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:' , X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_train:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    3946\n",
       "1    2546\n",
       "Name: e401k, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_test:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1692\n",
       "1    1091\n",
       "Name: e401k, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('y_train:', y_train.value_counts())\n",
    "display('y_test:', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some imbalance in target variable `e401k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier # for bagging decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB # to use in adaboost\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Pipelines and GridSearchCV\n",
    "[GridSearch + Pipeline multiple models reference](https://ryan-reilly.medium.com/gridsearch-pipelines-of-multiple-models-on-multiclass-classification-e9124b6ea2e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipelines\n",
    "logr_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('logr', LogisticRegression())\n",
    "])\n",
    "\n",
    "knc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('knc', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "dtc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('dtc', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "bagc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('bagc', BaggingClassifier()) # check if default is decision trees\n",
    "])\n",
    "\n",
    "rfc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "abc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('abc', AdaBoostClassifier()) # use with Extra Trees\n",
    "])\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "seeds=[1,2,3]\n",
    "\n",
    "logr_params = [{\n",
    "    'logr__class_weight':['balanced', {1:1.24, 0:0.8}],\n",
    "    'logr__solver':['liblinear', 'saga'],\n",
    "    'logr__penalty': ['l1','l2'],\n",
    "    'logr__C': [.1, .5, 1.],\n",
    "    'logr__random_state': seeds\n",
    "    #'logr__max_iter':[1_000, 5_000] # default=100\n",
    "}]\n",
    "\n",
    "knc_params = [{\n",
    "    'knc__n_neighbors':[3,5,9],\n",
    "    'knc__weights':['uniform','distance']\n",
    "}]\n",
    "\n",
    "dtc_params = [{\n",
    "    'dtc__random_state':seeds,\n",
    "    'dtc__splitter':['random','best'],\n",
    "    'dtc__max_depth':[None,3,5]\n",
    "}]\n",
    "\n",
    "bagc_params = [{\n",
    "    'bagc__random_state':seeds,\n",
    "    'bagc__n_estimators':[5,10]\n",
    "}]\n",
    "\n",
    "rfc_params = [{\n",
    "    'rfc__random_state':seeds,\n",
    "    'rfc__n_estimators':[100,150,200],\n",
    "    'rfc__max_depth':[None,3,5]\n",
    "}]\n",
    "\n",
    "abc_params = [{\n",
    "    'abc__base_estimator':[MultinomialNB()],\n",
    "    'abc__learning_rate':[1.,2.],\n",
    "    'abc__random_state':seeds\n",
    "}]\n",
    "\n",
    "svc_params = [{\n",
    "    'svc__C':[.01,.1,.5],\n",
    "    'svc__max_iter':[80_000, 100_000],\n",
    "    'svc__probability':[True],\n",
    "    'svc__random_state':seeds\n",
    "}]\n",
    "\n",
    "# cannot use standard scaler due to presence of negative values, \n",
    "# multinomialNB cannot accept negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'accuracy'\n",
    "\n",
    "logr_gs = GridSearchCV(logr_pipe,\n",
    "                       logr_params,\n",
    "                       scoring=score,\n",
    "                       cv=5,\n",
    "                       verbose=1)\n",
    "\n",
    "knc_gs = GridSearchCV(knc_pipe,\n",
    "                      knc_params,\n",
    "                      scoring=score,\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "dtc_gs = GridSearchCV(dtc_pipe,\n",
    "                      dtc_params,\n",
    "                      scoring=score,\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "bagc_gs = GridSearchCV(bagc_pipe,\n",
    "                       bagc_params,\n",
    "                       scoring=score,\n",
    "                       cv=5,\n",
    "                       verbose=1)\n",
    "\n",
    "rfc_gs = GridSearchCV(rfc_pipe,\n",
    "                      rfc_params,\n",
    "                      scoring=score,\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "abc_gs = GridSearchCV(abc_pipe,\n",
    "                      abc_params,\n",
    "                      scoring=score,\n",
    "                      cv=5,\n",
    "                      verbose=1)\n",
    "\n",
    "svc_gs = GridSearchCV(svc_pipe,\n",
    "                      svc_params,\n",
    "                      scoring=score,\n",
    "                      cv=5,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 04/08/2022, 14:45\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "CPU times: total: 14min 15s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "named_tuple = time.localtime() # get start_time\n",
    "time_string = time.strftime(\"%d/%m/%Y, %H:%M\", named_tuple) # format time print\n",
    "print(f'start time: {time_string}')\n",
    "\n",
    "# fit models\n",
    "grids = [logr_gs, knc_gs, dtc_gs, bagc_gs, rfc_gs, abc_gs, svc_gs]\n",
    "for pipe in grids:\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, accuracy_score, \n",
    "                             recall_score, precision_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy: 0.6078250154035736\n",
      "Elastic Net confusion matrix: \n",
      " [[1220  472]\n",
      " [ 502  589]]\n",
      "Elastic Net train accuracy: 0.646\n",
      "Elastic Net test accuracy: 0.650\n",
      "Elastic Net precision: 0.555\n",
      "Elastic Net recall: 0.540\n",
      "Elastic Net f1 score: 0.547\n",
      "Elastic Net specificity: 0.721\n",
      "Elastic Net ROC AUC: 0.693\n",
      "Elastic Net accuracy generalisation: -0.594 %\n",
      "Elastic Net Best Params: {'logr__C': 1.0, 'logr__class_weight': 'balanced', 'logr__penalty': 'l1', 'logr__random_state': 1, 'logr__solver': 'saga'}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "K-Neighbors confusion matrix: \n",
      " [[1296  396]\n",
      " [ 643  448]]\n",
      "K-Neighbors train accuracy: 0.717\n",
      "K-Neighbors test accuracy: 0.627\n",
      "K-Neighbors precision: 0.531\n",
      "K-Neighbors recall: 0.411\n",
      "K-Neighbors f1 score: 0.463\n",
      "K-Neighbors specificity: 0.766\n",
      "K-Neighbors ROC AUC: 0.624\n",
      "K-Neighbors accuracy generalisation: 12.660 %\n",
      "K-Neighbors Best Params: {'knc__n_neighbors': 9, 'knc__weights': 'uniform'}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "Decision Tree confusion matrix: \n",
      " [[1255  437]\n",
      " [ 454  637]]\n",
      "Decision Tree train accuracy: 0.696\n",
      "Decision Tree test accuracy: 0.680\n",
      "Decision Tree precision: 0.593\n",
      "Decision Tree recall: 0.584\n",
      "Decision Tree f1 score: 0.588\n",
      "Decision Tree specificity: 0.742\n",
      "Decision Tree ROC AUC: 0.717\n",
      "Decision Tree accuracy generalisation: 2.312 %\n",
      "Decision Tree Best Params: {'dtc__max_depth': 5, 'dtc__random_state': 1, 'dtc__splitter': 'best'}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "Bagged Decision Trees confusion matrix: \n",
      " [[1342  350]\n",
      " [ 629  462]]\n",
      "Bagged Decision Trees train accuracy: 0.975\n",
      "Bagged Decision Trees test accuracy: 0.648\n",
      "Bagged Decision Trees precision: 0.569\n",
      "Bagged Decision Trees recall: 0.423\n",
      "Bagged Decision Trees f1 score: 0.486\n",
      "Bagged Decision Trees specificity: 0.793\n",
      "Bagged Decision Trees ROC AUC: 0.651\n",
      "Bagged Decision Trees accuracy generalisation: 33.508 %\n",
      "Bagged Decision Trees Best Params: {'bagc__n_estimators': 10, 'bagc__random_state': 1}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "Random Forest confusion matrix: \n",
      " [[1394  298]\n",
      " [ 563  528]]\n",
      "Random Forest train accuracy: 0.700\n",
      "Random Forest test accuracy: 0.691\n",
      "Random Forest precision: 0.639\n",
      "Random Forest recall: 0.484\n",
      "Random Forest f1 score: 0.551\n",
      "Random Forest specificity: 0.824\n",
      "Random Forest ROC AUC: 0.724\n",
      "Random Forest accuracy generalisation: 1.309 %\n",
      "Random Forest Best Params: {'rfc__max_depth': 5, 'rfc__n_estimators': 150, 'rfc__random_state': 1}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "AdaBoost confusion matrix: \n",
      " [[1622   70]\n",
      " [ 983  108]]\n",
      "AdaBoost train accuracy: 0.621\n",
      "AdaBoost test accuracy: 0.622\n",
      "AdaBoost precision: 0.607\n",
      "AdaBoost recall: 0.099\n",
      "AdaBoost f1 score: 0.170\n",
      "AdaBoost specificity: 0.959\n",
      "AdaBoost ROC AUC: 0.617\n",
      "AdaBoost accuracy generalisation: -0.140 %\n",
      "AdaBoost Best Params: {'abc__base_estimator': MultinomialNB(), 'abc__learning_rate': 1.0, 'abc__random_state': 1}\n",
      "\n",
      "baseline accuracy: 0.6078250154035736\n",
      "Support Vector regressor confusion matrix: \n",
      " [[1526  166]\n",
      " [ 836  255]]\n",
      "Support Vector regressor train accuracy: 0.648\n",
      "Support Vector regressor test accuracy: 0.640\n",
      "Support Vector regressor precision: 0.606\n",
      "Support Vector regressor recall: 0.234\n",
      "Support Vector regressor f1 score: 0.337\n",
      "Support Vector regressor specificity: 0.902\n",
      "Support Vector regressor ROC AUC: 0.674\n",
      "Support Vector regressor accuracy generalisation: 1.292 %\n",
      "Support Vector regressor Best Params: {'svc__C': 0.5, 'svc__max_iter': 80000, 'svc__probability': True, 'svc__random_state': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = 3946/(2546+3946) # majority class is '0'\n",
    "\n",
    "grid_dict = {0: 'Elastic Net', 1: 'K-Neighbors',\n",
    "            2: 'Decision Tree', 3: 'Bagged Decision Trees',\n",
    "            4: 'Random Forest', 5: 'AdaBoost',\n",
    "            6: 'Support Vector regressor'}\n",
    "\n",
    "for i, model in enumerate(grids):\n",
    "    preds = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    print(f'baseline accuracy: {baseline}')\n",
    "    print(f'{grid_dict[i]} confusion matrix: \\n {confusion_matrix(y_test, preds)}')\n",
    "    print(f'{grid_dict[i]} train accuracy: {model.score(X_train,y_train):.3f}')\n",
    "    print(f'{grid_dict[i]} test accuracy: {model.score(X_test,y_test):.3f}') # accuracy score\n",
    "    print(f'{grid_dict[i]} precision: {precision_score(y_test, preds):.3f}') #(tp / (tp + fp))\n",
    "    print(f'{grid_dict[i]} recall: {recall_score(y_test, preds):.3f}') # sensitivity (tp / (tp + fn))\n",
    "    print(f'{grid_dict[i]} f1 score: {f1_score(y_test, preds):.3f}') # aim precision = recall\n",
    "    print(f'{grid_dict[i]} specificity: {(tn / (tn + fp)):.3f}')\n",
    "    print(f'{grid_dict[i]} ROC AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:,1]):.3f}')\n",
    "    print(f'{grid_dict[i]} accuracy generalisation: {(((model.score(X_train,y_train)-model.score(X_test,y_test))*100)/model.score(X_train,y_train)):.3f} %')\n",
    "    print(f'{grid_dict[i]} Best Params: {model.best_params_}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The best classifier based on ROC AUC score is `Random Forest classifier`. It also has good generalisation of 1.309 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_mat = confusion_matrix(y_test, rfc_gs.predict(X_test))\n",
    "ConfusionMatrixDisplay(confusion_mat, display_labels=['not_eligible','eligible']).plot(cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('True values');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "**False positives** are values that are predicted as eligible but actually not eligible.\n",
    "\n",
    "**False negatives** are values that are predicted as not eligible but actually eligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "For this situation, it is important to ensure that all who are eligible for 401k are flagged, and not missed, because that would be missed business opportunities. \n",
    "So this would mean I would rather minimise false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "I would optimise specificity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer: \n",
    "f1 score is a weighted average of precision and recall scores. \n",
    "\n",
    "Precision measures true positives out of all the predicted positives(includes false positives), and recall measures predicted positives out of all the positives(includes false negatives), so in a way, f1 score can be inferred if false positives and false negatives are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net train f1 score: 0.539\n",
      "Elastic Net test f1 score: 0.547\n",
      "\n",
      "K-Neighbors train f1 score: 0.595\n",
      "K-Neighbors test f1 score: 0.463\n",
      "\n",
      "Decision Tree train f1 score: 0.605\n",
      "Decision Tree test f1 score: 0.588\n",
      "\n",
      "Bagged Decision Trees train f1 score: 0.967\n",
      "Bagged Decision Trees test f1 score: 0.486\n",
      "\n",
      "Random Forest train f1 score: 0.565\n",
      "Random Forest test f1 score: 0.551\n",
      "\n",
      "AdaBoost train f1 score: 0.191\n",
      "AdaBoost test f1 score: 0.170\n",
      "\n",
      "Support Vector regressor train f1 score: 0.363\n",
      "Support Vector regressor test f1 score: 0.337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(grids):\n",
    "    trn_preds = model.predict(X_train)\n",
    "    tst_preds = model.predict(X_test)\n",
    "    print(f'{grid_dict[i]} train f1 score: {f1_score(y_train, trn_preds):.3f}')\n",
    "    print(f'{grid_dict[i]} test f1 score: {f1_score(y_test, tst_preds):.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "Based on training f1-score and testing f1-score, there is evidence of overfitting for K-Neighbors, Bagged Decision Trees, AdaBoost models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I pick `Random Forest classifier` as my final model based on best ROC AUC score among models. It also has good generalisation of 1.309 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer:\n",
    "\n",
    "For classification problem, if I had more time, I would \n",
    "* explore other models\n",
    "* explore different feature selection / combinations\n",
    "* explore feature engineering by feature interaction\n",
    "* explore new features (get data of new features) that may have more predictive value\n",
    "* explore hyperparameter fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
